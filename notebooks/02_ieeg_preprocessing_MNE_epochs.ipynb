{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dce6adc",
   "metadata": {},
   "source": [
    "# Part 2: Preprocessing intracranial EEG using MNE-python - Epochs!\n",
    "\n",
    "*WIRED 2024*  \n",
    "[Maansi Desai, PhD](https://maansidesai.github.io/)  \n",
    "Postdoctoral Researcher in the [Hamilton Lab](https://slhs.utexas.edu/research/hamilton-lab)\n",
    "<br>\n",
    "Department of Speech, Language, and Hearing Sciences \n",
    "<br>\n",
    "The University of Texas at Austin  \n",
    "\n",
    "This is part two of the notebooks. Please first run through [`01_ieeg_preprocessing_MNE.ipynb`](01_ieeg_preprocessing_MNE.ipynb) before running this. In this portion of the tutorial, you will learn about epoching your data. Epoched data allows you to calculate averaged responses to events of interest (event-related potentials). We will do this based on the provided annotations of speech vs. music, as well as additional annotations that are available in the [Berezutskaya et al.](https://www.nature.com/articles/s41597-022-01173-0) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb884ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from mne_bids import read_raw_bids, print_dir_tree, BIDSPath\n",
    "from mne_bids.path import get_bids_path_from_fname\n",
    "from bids import BIDSLayout\n",
    "from ecog_preproc_utils import transformData\n",
    "import bids \n",
    "\n",
    "print(mne.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f2b5d",
   "metadata": {},
   "source": [
    "## Load BIDS iEEG dataset\n",
    "\n",
    "\n",
    "If you have ran [`01_ieeg_preprocessing_MNE`](01_ieeg_preprocessing_MNE), then you should already have downloaded the necessary datasets locally. \n",
    "\n",
    "For this notebook we will use data from: \n",
    " - `sub-W2`, `iemu`, `B8`, `timit5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the example participant's data that we will load for the tutorial,\n",
    "# but there are more options.\n",
    "\n",
    "# In this example, we will be loading data from subj W2 to plot evoked responses to sentences from the TIMIT speech corpora.\n",
    "subj = 'W2'\n",
    "sess = 'iemu'\n",
    "task = 'timit5'\n",
    "acq = 'B8'\n",
    "run = '01'\n",
    "\n",
    "parent_dir = 'data/ds004993'\n",
    "bids_path = BIDSPath(\n",
    "    root=parent_dir, subject=subj, session=sess, task=task, acquisition=acq, run=run, datatype=\"ieeg\"\n",
    ")\n",
    "print(bids_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0da51ef",
   "metadata": {},
   "source": [
    "## Load the iEEG data\n",
    "\n",
    "First, we will choose the relevant subject, session, task, acquisition, and run. Note that if you wish to change these variables, you may need to download the data yourself.\n",
    "\n",
    "To show the capabilities of BIDS and contrast to when we don't use BIDS, we'll load the data in two ways. The data structure using BIDS will be called `raw`, the data structure without BIDS will be `raw_nobids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4431118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data and extract parameters from BIDS files\n",
    "raw = read_raw_bids(bids_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f183f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the data into memory and print some information about it. The \n",
    "# info structure contains a lot of helpful metadata about number of channels,\n",
    "# sampling rate, data types, etc. It can also contain information about the\n",
    "# participant and date of acquisition, however, this dataset has been anonymized.\n",
    "raw.load_data()\n",
    "raw.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadbb278",
   "metadata": {},
   "source": [
    "# Calculate the high gamma transform of your data\n",
    "\n",
    "Now we will take the raw, preprocessed data, and convert to high gamma analytic amplitude for further analysis. The high gamma analytic amplitude is used in many papers as a proxy for multi-unit firing (see [Ray and Maunsell, PLoS Biology 2011](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1000610)).\n",
    "\n",
    "This particular version of the high gamma transform uses the same procedure as used in [Hamilton et al. 2018](cell.com/current-biology/pdf/S0960-9822(18)30461-5.pdf) and [Hamilton et al. 2021](https://www.cell.com/cell/pdf/S0092-8674(21)00878-3.pdf). The basic idea is to take 8 bands within the 70-150 Hz range, calculate the Hilbert transform, then take the analytic amplitude of that signal and average across the 8 bands. This form of averaging results in higher SNR than one band between 70-150 Hz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the iEEG channels for high gamma\n",
    "raw_ieeg = raw.copy()\n",
    "raw_ieeg.pick_types(seeg=True)\n",
    "raw_ieeg.anonymize()\n",
    "\n",
    "notch_freqs = list(np.arange(raw.info['line_freq'], raw.info['lowpass'], step=raw.info['line_freq']))\n",
    "# Get the high gamma data\n",
    "# Generally, do a CAR if you have widespread coverage over multiple\n",
    "# areas (not just one sensory area)\n",
    "# If you have limited coverage, you may choose to do no CAR or choose\n",
    "# to reference to one specific channel.\n",
    "ieeg_dir = f'{parent_dir}/sub-{subj}/ses-{sess}/ieeg/'\n",
    "hgdat = transformData(raw_ieeg, ieeg_dir, band='high_gamma', notch=True, CAR=True,\n",
    "                      car_chans='average', log_transform=True, do_zscore=True,\n",
    "                      hg_fs=100, notch_freqs=notch_freqs, ch_types='seeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d961f2",
   "metadata": {},
   "source": [
    "# Plotting evoked data\n",
    "\n",
    "Now that we have some preprocessed data, let's plot the differences between experimental conditions. To do this, we will need the events timings, which are included in the `events.tsv` file. In this case, the events correspond to blocks of music and speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bc5c0e",
   "metadata": {},
   "source": [
    "## Loading events\n",
    "\n",
    "Now we will load events from the .tsv file to plot evoked responses to music and speech events. First I'll show you how to do this by creating an MNE events array, next I'll show you how to derive them from the annotations. This first method could also be used with non-BIDS datasets if you have the onset and duration and trial information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397699df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple way of loading a tab-delimited file, and is not specific to\n",
    "# MNE python. We're using the library pandas, which you may also find very\n",
    "# helpful in other applications.\n",
    "event_file = f'{ieeg_dir}/sub-{subj}_ses-{sess}_task-{task}_acq-{acq}_run-01_events.tsv'\n",
    "event_df = pd.read_csv(event_file, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c616eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the contents of this dataframe. \n",
    "event_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bdf27",
   "metadata": {},
   "source": [
    "## Convert event times to samples\n",
    "\n",
    "Now these event times are in seconds, not samples, so we have to convert them for use with MNE python's epochs constructor. Let's do that here. \n",
    "\n",
    "The times here are in seconds, and sampling rate is in units of Hz (samples/sec), so to get samples, we just multiply the amount of time by the sampling rate.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mbox{number of samples} &=& \\mbox{time }  \\times \\mbox{sampling rate}\\\\\n",
    "\\mbox{(samples)} &=& \\mbox{(s) }  \\times \\mbox{(samples/sec)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "We also cast these as integers since data samples are discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "onset_samp = [int(onset*hgdat.info['sfreq']) for onset in event_df.onset]\n",
    "dur_samp = [int(dur*hgdat.info['sfreq']) for dur in event_df.duration]\n",
    "ev_id = [int(e) for e in event_df.value]\n",
    "\n",
    "\n",
    "eve = np.vstack((onset_samp, dur_samp, ev_id)).T\n",
    "eve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996b3d3",
   "metadata": {},
   "source": [
    "## Another way...\n",
    "\n",
    "So actually, because we already had these particular events as annotations, we could have also done this a simpler way, but the method above also works for other events that are stored in tsv files without becoming annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195dba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.annotations.to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e1311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could also do this with the `raw` object\n",
    "#events = mne.events_from_annotations(hgdat, event_id='auto')\n",
    "\n",
    "#you can load event_ids from the BIDS data which stored all of the markers which is in a sampling rate of 512 Hz...because getting high gamma \n",
    "#from BIDS didn't work as expected...\n",
    "events = mne.events_from_annotations(raw, event_id='auto')\n",
    "events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd819c75",
   "metadata": {},
   "source": [
    "## Create an epochs object\n",
    "\n",
    "Now if we want to plot our data by epoch type, we can use the mne Epochs class. This allows us to parse our data according to these events and plot evoked activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ed235",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = -0.2  # How much time to account for before the event of interest\n",
    "tmax = 0.5   # How much time to account for after the event of interest\n",
    "\n",
    "events = eve\n",
    "#event_id = events[1]  # This is the speech event ID\n",
    "event_id = events[:, 2][1]\n",
    "\n",
    "# Here we take events[0] because those are the timings, whereas events[1] has\n",
    "# the information about event type. If you just have a list of timings,\n",
    "# you don't need to index the events in this way.\n",
    "epochs = mne.Epochs(hgdat, events=events, tmin=tmin, tmax=tmax, event_id=event_id) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will just plot the average across all channels. This is a bit\n",
    "# weird to do with iEEG because this is across a lot of different brain\n",
    "# areas, but it's still possible.\n",
    "epochs.plot_image(combine='mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4a3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about plotting a particular electrode? This is one that\n",
    "# appears to be on the STG based on the image above\n",
    "ch_name = hgdat.info['ch_names'][49]\n",
    "print(f'plotting channel for {ch_name}')\n",
    "epochs.plot_image(picks=[hgdat.info['ch_names'][49]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90528d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epochs(epochs, nchans, ch_names, color='b', label='spkr', show=True, vmin_max=None):\n",
    "    '''\n",
    "    Function that plots the averaged epoched data for each channel as a grid so you can \n",
    "    see all channels at once.\n",
    "    \n",
    "    Inputs:\n",
    "        epochs [obj] : MNE epochs object\n",
    "        nchans [int] : number of channels to plot\n",
    "        ch_names [list] : channel names \n",
    "        color [str, hex, tuple]: color for ERP traces\n",
    "        label [str] : label for the ERP (could be epoch type/annotation type) \n",
    "        show [bool] : whether to show the figure or not\n",
    "        vmin_max [list] : list of ylim min and max, e.g. [-0.5, 0.5]\n",
    "    '''\n",
    "    \n",
    "    # Get the data as an array\n",
    "    eps = epochs.get_data()\n",
    "    \n",
    "    # Find the maximum across the whole dataset, helps with scaling the plots\n",
    "    emax = np.abs(epochs.average().data).max()\n",
    "    \n",
    "    # Determine how many rows and columns we'll need in our subplots grid\n",
    "    # based on the number of channels. \n",
    "    nrows = int(np.floor(np.sqrt(nchans)))\n",
    "    ncols = int(np.ceil(nchans/nrows))\n",
    "    \n",
    "    # Loop through all electrode channels\n",
    "    for ch in np.arange(nchans):\n",
    "        plt.subplot(nrows, ncols, ch+1)\n",
    "        \n",
    "        # Get the average response across trials for this particular channel\n",
    "        erp = eps[:,ch,:].mean(0)\n",
    "        \n",
    "        # Get the standard error across trials\n",
    "        erpstderr = eps[:,ch,:].std(0)/np.sqrt(eps.shape[0])\n",
    "        \n",
    "        # Plot transparent shaded standard error in the [color] you choose\n",
    "        ybottom = erp - erpstderr\n",
    "        ytop = erp + erpstderr\n",
    "        plt.fill_between(epochs.times, ybottom.ravel(), ytop.ravel(),\n",
    "                         alpha=0.5, color=color)\n",
    "        \n",
    "        # Plot the average epoch on top in the same color\n",
    "        plt.plot(epochs.times, erp, color=color, label=label)\n",
    "        \n",
    "        # Plot the x and y origins\n",
    "        plt.axvline([0], color='k', linewidth=0.5)\n",
    "        plt.axhline([0], color='k', linewidth=0.5)\n",
    "        \n",
    "        # If we haven't explicitly set ylimits with vmin/vmax, use \n",
    "        # the maximum of the data and 50% more so the whole thing \n",
    "        # fits nicely \n",
    "        if vmin_max is None:\n",
    "            plt.gca().set_ylim([-emax*1.5, emax*1.5])\n",
    "        else:\n",
    "            plt.gca().set_ylim([vmin_max[0], vmin_max[1]])\n",
    "            \n",
    "        # Only show the ticks for the 0th plot, otherwise this gets\n",
    "        # hard to see/read\n",
    "        if ch != 0:\n",
    "            plt.gca().set_xticks([])\n",
    "            plt.gca().set_yticks([])\n",
    "        else:\n",
    "            plt.ylabel('Z-score')\n",
    "        \n",
    "        # Write the name of the channel in the plot -- you could also\n",
    "        # use plt.title() but sometimes that makes everything look\n",
    "        # a little squashed\n",
    "        plt.text(0.5, 0.25, ch_names[ch], \n",
    "            horizontalalignment='center', verticalalignment='center',\n",
    "            transform=plt.gca().transAxes, fontsize=8)\n",
    "    \n",
    "    # Plot ticks at meaningful times (the min, 0, and max in seconds)\n",
    "    plt.gca().set_xticks([epochs.tmin, 0, epochs.tmax])\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.legend()\n",
    "    #plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f46af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plot_epochs(epochs, len(hgdat.info['ch_names']), hgdat.info['ch_names'], label='speech', show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d4649",
   "metadata": {},
   "source": [
    "# Now let's try epoching based on phoneme information. Work together by yourself or with your neighbor. \n",
    "\n",
    "### Make sure you have followed the instruction on the [github repo](https://github.com/maansidesai/WIRED-2024-Paris?tab=readme-ov-file) to download the [Berezutskaya iEEG and fMRI data from movie viewing](https://openneuro.org/datasets/ds003688/versions/1.0.7) dataset for `sub-06`. \n",
    "\n",
    "There are many other subjects in this BIDS dataset that you can eventually use for your own analysis or visualization. However for the purpose of this exercise, we have request you all to download `sub-06` due to the large size of the complete Berezutskaya et al. dataset.\n",
    "\n",
    "\n",
    "Open [`04_ieeg_breakout_MNE.ipynb`](04_ieeg_breakout_MNE.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
