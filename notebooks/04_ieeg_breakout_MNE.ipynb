{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e33accc-933d-4986-b9e0-ab937e24238c",
   "metadata": {},
   "source": [
    "# Part 4: Putting your knowledge to the test - loading BIDS data, preprocessing, and plotting evoked data!\n",
    "\n",
    "*WIRED 2024*  \n",
    "[Maansi Desai, PhD](https://maansidesai.github.io/)  \n",
    "Postdoctoral Researcher in the [Hamilton Lab](https://slhs.utexas.edu/research/hamilton-lab)\n",
    "<br>\n",
    "Department of Speech, Language, and Hearing Sciences \n",
    "<br>\n",
    "The University of Texas at Austin  \n",
    "\n",
    "This is part four of the notebooks. Please first run through [`01_ieeg_preprocessing_MNE.ipynb`](01_ieeg_preprocessing_MNE.ipynb) and [`02_ieeg_preprocessing_MNE_epochs.ipynb`](01_ieeg_preprocessing_MNE_epochs.ipynb)before running this. \n",
    "\n",
    "You should now have the tools to try and conduct this entire process for yourself. Feel free to work by yourself or with your neighbor in the section of the workshop.  \n",
    "\n",
    "You will write your code in the `### CODE HERE ###` segments of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2d0fc2-85ad-42da-8906-fcf6b2a3cf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from mne_bids import read_raw_bids, print_dir_tree, BIDSPath\n",
    "from mne_bids.path import get_bids_path_from_fname\n",
    "from bids import BIDSLayout\n",
    "from ecog_preproc_utils import transformData\n",
    "import bids \n",
    "from plt_eps import plot_epochs\n",
    "\n",
    "print(mne.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d28a34-bd0e-4564-94c0-b7be4b65ff0b",
   "metadata": {},
   "source": [
    "## Load BIDS iEEG dataset\n",
    "\n",
    "Follow instructions on the [README](https://github.com/maansidesai/WIRED-2024-Paris?tab=readme-ov-file) to ensure you are downloding the correct dataset for `sub-06`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f707eb9-3584-4fa3-b12b-7e6cf1aca90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the example participant's data that we will load for the tutorial,\n",
    "# but there are more options.\n",
    "subj = '06'\n",
    "sess = 'iemu'\n",
    "task = 'film'\n",
    "acq = 'clinical'\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f79859",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'data/ds003688'\n",
    "bids_path = BIDSPath(\n",
    "    root=parent_dir, subject=subj, session=sess, task=task, acquisition=acq, run=run, datatype=\"ieeg\"\n",
    ")\n",
    "print(bids_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8842ec5d-aee7-4aac-b0b6-848d65b96dbd",
   "metadata": {},
   "source": [
    "## BIDS layout\n",
    "\n",
    "We can use `pybids` to show a little bit about the files in this BIDS dataset. We won't get as much into this, but if you'd like to try this tutorial on your own you may wish to delve into this more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23bb10d-2ced-4435-82e1-477310ed302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = BIDSLayout(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b4f5f-7a0e-4a4f-a946-59f479e403e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layout.get_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168faad8-800e-4719-8a6a-469f0a2845d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = layout.get()\n",
    "print(\"There are {} files in the layout.\".format(len(all_files)))\n",
    "print(\"\\nThe first 10 files are:\")\n",
    "all_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf919b-7bdf-4d1e-99bb-e34ecbfe7970",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(parent_dir, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bbdb7-799b-4558-bc9b-fc34335c8b17",
   "metadata": {},
   "source": [
    "## Read data and extract parameters from BIDS files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c026761-c8d4-43b0-82e9-be142fe4df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw = ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c3acd-be32-49a2-86f6-958588b05d67",
   "metadata": {},
   "source": [
    "## Plot the raw data and inspect the data for any artifacts you may need to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424983e-d9f2-45ed-854c-d277dc2560b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193a5dfd-c602-454d-941a-e9688be4f371",
   "metadata": {},
   "source": [
    "## Plot the power spectrum to identify noisy channels you may need to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f89b45-534b-43fa-b2c6-4dbfdc894d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1226e085-d24c-4ea0-b995-c45e8d64a0c1",
   "metadata": {},
   "source": [
    "## Conduct a common average reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0727bd0-e0d3-4512-ba59-50fd72473379",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ref = raw.copy()\n",
    "\n",
    "### THE REST OF THE CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc4d9d-1611-4527-a1b3-028092eb00e9",
   "metadata": {},
   "source": [
    "## Calculate the high gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadd31b-1cba-462b-a59e-7c804095e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the iEEG channels for high gamma\n",
    "raw_ieeg = raw.copy()\n",
    "raw_ieeg.pick_types(ecog=True)\n",
    "raw_ieeg.anonymize()\n",
    "\n",
    "notch_freqs = list(np.arange(raw.info['line_freq'], raw.info['lowpass'], step=raw.info['line_freq']))\n",
    "# Get the high gamma data\n",
    "# Generally, do a CAR if you have widespread coverage over multiple\n",
    "# areas (not just one sensory area)\n",
    "# If you have limited coverage, you may choose to do no CAR or choose\n",
    "# to reference to one specific channel.\n",
    "\n",
    "\n",
    "hgdat = transformData(raw_ieeg, ieeg_dir, band='high_gamma', notch=True, CAR=True,\n",
    "                      car_chans='average', log_transform=True, do_zscore=True,\n",
    "                      hg_fs=100, notch_freqs=notch_freqs, ch_types='ecog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb8e15-ec40-40d5-9f39-a0b9ab33a6f3",
   "metadata": {},
   "source": [
    "## Load your event file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ad625-8ac1-4f18-b7d3-f9e4ea0bae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "#Hint: you can use mne.events_from_annotations or the .tsv file. Remember what sampling rate your are working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3773a-0926-49ae-976a-40dc26cacc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmin = -0.2  # How much time to account for before the event of interest\n",
    "tmax = 0.5   # How much time to account for after the event of interest\n",
    "event_id = events[1]['speech']  # This is the speech event ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fff4f-3822-43d7-98aa-786e41287821",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ### -- HINT: use mne.Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bd3a1-ee75-400f-995b-69cefd50b23b",
   "metadata": {},
   "source": [
    "## Do some plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730bacad-54ab-4072-bb37-cb43cc11ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: plot epochs across all regions of the brain (yes, weird to do but just as an exercise\n",
    "#2: take a specific channel and plot the epochs\n",
    "\n",
    "#1 - ### YOUR CODE HERE ###\n",
    "\n",
    "#2 - ### YOUR CODE HERE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3290ca-e3ff-4592-9cbf-0ffb5581ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use plt_eps python function to plot across all channels for the speech responses:\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c93ba-205a-4c16-83ff-7b66c004ef40",
   "metadata": {},
   "source": [
    "## Now for something a bit more challenging...\n",
    "\n",
    "So far, in the tutorial and in the exercises thus far, we looked at plotting the evoked responses to the onset of sentence information. But now, what do these evoked responses look like if we plot based on the onset of each `word`?\n",
    "\n",
    "In addition to the \"speech\" vs \"music\" gross-level annotations, the researchers have provided information about the onset and offset of different types of information in the sound as well as the video. You can look in the `stimuli` folder to see what types of annotations are provided, but in general, these include word-level, syllable-level, sentence-level, and specific talkers as well as some other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168d3eb6-01cc-4abe-8f6d-d08f179967cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEPS:\n",
    "\n",
    "# 1 - Get the word times:\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "# 2 - #Get the sample that corresponds to the start of the task, since we will need to offset all the stimulus time labels from that\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "word_events = []\n",
    "# 3 - Loop through the times for each word event and convert to samples and save to the list `word_events`\n",
    "for idx, row in word_times.iterrows():\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    # Append this event to our events list\n",
    "    word_events.append([onset_sample, duration_sample, 1])\n",
    "\n",
    "# 4  Now create the epochs object again. Note that we don't need to index the `word_events`\n",
    "#because it is already a list in the correct format\n",
    "epochs_words = ### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aebe99-d6e1-4320-8e36-118bc8e2c12e",
   "metadata": {},
   "source": [
    "### Now plot the word epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7346332-1ec1-4999-86fb-a83d26db1939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average across all channels (yes, again this is weird but this is just an exercise)\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac89ed1-6ea3-43e9-a782-1eb875ffb59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one subplot for each channel as we had done before\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee4a9fb-6493-4b54-a8e7-de5cb9acf52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use plt_eps python function to plot across all channels for the speech responses:\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
